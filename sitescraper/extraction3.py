# -*- coding: utf-8 -*-
"""Copy of Copy of Copy of Untitled0.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1S3jB1Lkz2b4vwkwQ9dz0SA4ep2BsssiR
"""

import spacy
from newspaper import Article
from newsapi import NewsApiClient
import newspaper
import sys
import glob
import json
from gensim.scripts.glove2word2vec import glove2word2vec
# Gensim contains word2vec models and processing tools
from gensim.models import KeyedVectors
from gensim.test.utils import datapath, get_tmpfile
import en_core_web_sm
from collections import Counter
from spacy import displacy
import networkx as nx
from sklearn.metrics.pairwise import cosine_similarity
from nltk.corpus import stopwords
from nltk.tokenize import sent_tokenize
import re
import nltk
import pandas as pd
import numpy as np


# Commented out IPython magic to ensure Python compatibility.
nltk.download('punkt')  # one time execution
project_path = "D:\\Workspace\\github\\be_my_voice\\sitescraper\\"
# %cd  "/content/drive/My Drive/DLCP/Fake News Challenge/"

print("project directory set to " + project_path)

word_embeddings = {}
f = open("D:\\Workspace\\github\\be_my_voice\\sitescraper\\" +
         'glove.6B.300d.txt', encoding='utf-8')
for line in f:
    values = line.split()
    word = values[0]
    coefs = np.asarray(values[1:], dtype='float32')
    word_embeddings[word] = coefs
f.close()
print("Glove setup complete")

nltk.download('stopwords')

# function to remove stopwords

stop_words = stopwords.words('english')


def remove_stopwords(sen):
    sen_new = " ".join([i for i in sen if i not in stop_words])
    return sen_new


spacy_nlp = spacy.load('en_core_web_sm')
spacy.load('en_core_web_sm')
nlp = en_core_web_sm.load()

print("configured Spacy")

path = "D:\\Workspace\\github\\be_my_voice\\sitescraper\\"

glove_file = datapath(path + 'glove.6B.300d.txt')  # This is a GloVe model
tmp_file = get_tmpfile(path + 'word2vec.glove.6B.300d.txt')

# Converting the GloVe file into a Word2Vec file
glove2word2vec(glove_file, tmp_file)
model = KeyedVectors.load_word2vec_format(tmp_file)

nltk.download('brown')  # download the corous
nltk.download('averaged_perceptron_tagger')  # Download the tagger

print("NLTK setup complete")


def list2string(data):
    return " ".join(map(str, data))


def getwordgroup(sentence, searchword, maxright=0, expectstring=False):
    words = sentence.split(" ")
    searchresult = []
    if searchword in words:
        indexofword = words.index(searchword)
        # print("Found {" + searchword + "} at position " + str(indexofword + 1))
        if maxright > 0:
            if (indexofword + maxright) >= len(words):
                # print("specified right limit exceed length, can use additional " +
                #      str(len(words) - words.index(searchword) - 1) + " word(s)")
                searchresult = [words[ind]
                                for ind in list(range(indexofword, len(words)))]
            else:
                # print("fetching " + str(maxright) +
                #      " word(s) right of position " + str(indexofword+1))
                searchresult = [
                    words[ind]
                    for ind in list(
                        range(indexofword, indexofword + maxright + 1))]
        else:
            if (indexofword + maxright) <= 0:
                # print(
                #    "specified left limit is below zero, can use additional " +
                #    str(words.index(searchword)) + " word(s)")
                searchresult = [words[ind]
                                for ind in list(range(0, indexofword + 1))]
            else:
                # print("fetching " + str(maxright) +
                #       " word(s) before position " + str(indexofword + 1))
                searchresult = [
                    words[ind]
                    for ind in list(
                        range(indexofword + maxright, indexofword + 1))]
    # else:
        # print("No matching word found")
    if expectstring:
        searchresult = list2string(searchresult)
    # print(searchresult)
    return searchresult


def getwordgroupby(sentence, searchstring, maxright=0, expectstring=True):
    searchwords = searchstring.split(" ")
    # print(searchwords)
    searchresult = ""
    if len(searchwords) <= 1:
        # print("performing standard search")
        searchresult = getwordgroup(
            sentence, searchstring, maxright, True)
    else:
        # print("search string has multiple word")
        if maxright > 0:
            # print("using last word to search")
            searchresult = getwordgroup(
                sentence, searchwords[-1], maxright, True)
            # print(list2string(searchwords[:-1]))
            searchresult = list2string(searchwords[:-1]) + " " + searchresult
        else:
            # print("using first word to search")
            searchresult = getwordgroup(
                sentence, searchwords[0], maxright, True)
            searchresult = searchresult + " " + list2string(searchwords[1:])
    if not expectstring:
        searchresult = searchresult.split(" ")
    # print(searchresult)
    return searchresult


apikey = '38b178162f2e497cb564c6a6dafc6c9a'
query = "poaching and rhino"


def processarticle(url):
    try:
        article = Article(url)
        article.download()
        article.parse()
        # print(article.text)
        return article.text
    except:
        return ""


def processurls(jsonData):
    processed = []
    print("Processing " + str(len(jsonData["articles"])))
    for article in jsonData["articles"]:
        try:
            print("Scrapping " + article["url"])
            article["fullarticle"] = processarticle(article["url"])
            processed.append(article)
        except:
            print("unable to process " + article["url"])
    # source, author, content, description, fullarticle, publishedAt, title, url, urlToImage
    data = pd.DataFrame.from_dict(processed)
    return data


def process(art):
    print("processing " + str(art))
    sentences = []
    for s in getattr(art, 'fullarticle'):
        sentences.append(sent_tokenize(s))

    sentences = [y for x in sentences for y in x]  # flatten list

    clean_sentences = pd.Series(sentences).str.replace("[^a-zA-Z]", " ")

    # make alphabets lowercase
    clean_sentences = [s.lower() for s in clean_sentences]

    clean_sentences = [remove_stopwords(r.split()) for r in clean_sentences]

    print("generating vectors")
    sentence_vectors = []
    for i in clean_sentences:
        if len(i) != 0:
            v = sum([word_embeddings.get(w, np.zeros((300,)))
                     for w in i.split()])/(len(i.split())+0.001)
        else:
            v = np.zeros((300,))
        sentence_vectors.append(v)

    sim_mat = np.zeros([len(sentences), len(sentences)])

    for i in range(len(sentences)):
        for j in range(len(sentences)):
            if i != j:
                sim_mat[i][j] = cosine_similarity(sentence_vectors[i].reshape(
                    1, 300), sentence_vectors[j].reshape(1, 300))[0, 0]

    nx_graph = nx.from_numpy_array(sim_mat)
    scores = nx.pagerank(nx_graph)

    ranked_sentences = sorted(((scores[i], s)
                               for i, s in enumerate(sentences)), reverse=True)

    article_length = len(ranked_sentences)
    print("Article length: " + str(article_length))

    if article_length < 6:
        summary_length = article_length
    else:
        summary_length = article_length//3

    # print(summary_length)

    summary = ''
    for i in range(summary_length):
        summary = print(ranked_sentences[i][1])
    # print(summary)

    animal_found = []
    animal_volume = []
    animal_found_loc = []
    animal_found_city = []
    animal_money = []

    list_of_probable_word = ['seize', 'kill', 'carcass', 'poaching']
    all_word_list = []
    for i in list_of_probable_word:
        all_word_list += model.most_similar(positive=[i])

    animal_list = ['Tiger', 'Elephant', 'Rhino', 'rhino horn']

    summarized_sent = ''
    for i in ranked_sentences:
        summarized_sent += str(i[1])
    print(summarized_sent)

    for word in all_word_list:
        for individual_sent in ranked_sentences:
            if word[0].lower() in individual_sent[1].lower():
                # print(individual_sent[1])
                doc = nlp(individual_sent[1])
                name_entity_lisgt = [(X.text, X.label_) for X in doc.ents]
                # print(name_entity_lisgt)

                # Find animal Name

                for animal in animal_list:
                    if animal.lower() in individual_sent[1].lower():
                        animal_found.append(animal)

                for entity in name_entity_lisgt:
                    if 'CARDINAL' == entity[1]:
                        # print (entity[0])
                        getwordgroup(individual_sent[1], entity[0], -1)
                        # type(indices)
                        # print(individual_sent[1].split()[(indices[0]-1):(indices[0]+2)])
                        animal_volume.append(getwordgroup(
                            individual_sent[1], entity[0], -1))

                    if 'FAC' == entity[1]:
                        animal_found_loc.append(entity[0])

                    if 'GPE' == entity[1]:
                        animal_found_city.append(entity[0])

                    if 'QUANTITY' == entity[1]:
                        animal_volume.append(entity[0])

                    if 'MONEY' == entity[1]:
                        animal_money.append(entity[0])

    # print ( "-----------------")
    print(animal_found)
    print(animal_volume)
    print(animal_found_loc)
    print(animal_found_city)
    print(animal_money)

    # article["summary"] = summary
    # article["animal_found"] = animal_found
    # article["animal_volume"] = animal_volume
    # article["animal_found_loc"] = animal_found_loc
    # article["animal_found_city"] = animal_found_city
    # article["animal_money"] = animal_money

    return


df = []

print("searching for " + query)
api = NewsApiClient(api_key=apikey)
newsjson = api.get_everything(q=query, page_size=1, sort_by='relevancy')
# print(newsjson)
try:
    df = processurls(newsjson)
except:
    print("unable to process query")
print(df)


# loop dataframe
for searchData in df.itertuples(index=False):
    process(searchData)

print(df)
